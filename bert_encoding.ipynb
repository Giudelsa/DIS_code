{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and open the data\n",
    "1. documents\n",
    "2. queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_path = '/Users/giudittadelsarto/Desktop/DIS_Project/DIS_code/truncated_documents.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the documents\n",
    "with open(document_path) as json_file:\n",
    "    documents = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries_path = '../dis-project-1-document-retrieval/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training queries\n",
    "train_queries = pd.read_csv(train_queries_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the document given its docid\n",
    "def get_doc_by_id(doc_id):\n",
    "    return next(doc for doc in documents if doc['docid'] == doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_langauge(lang_id):\n",
    "    \"\"\"\n",
    "    Returns the language name given the language id in order to pass it to the stopwords function\n",
    "    \"\"\"\n",
    "    cases = {\n",
    "        'en' : 'english',\n",
    "        'de' : 'german',\n",
    "        'fr' : 'french',\n",
    "        'es' : 'spanish',\n",
    "        'it' : 'italian',\n",
    "        'ar' : 'arabic',\n",
    "        'ko' : 'korean'\n",
    "    }\n",
    "    return cases.get(lang_id, 'unknown') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, language_id):\n",
    "    \"\"\"\n",
    "    Preprocess the text by removing stopwords, stemming and lemmatizing the text\n",
    "    \"\"\"\n",
    "    # remove punctuation\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words(get_langauge(language_id)))\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cosine_similarity(a, b):\n",
    "#     return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ba1ecca3a14be09e9871ed1a133238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9102ff2f2bc74c60bb49ad2c89ee4a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb97fa7babd1473a90afd99b6ac9ac39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d636dce8a9d44398115881de9866d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2474115cdf4c32aee7de08e5195c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6bc32a91b24e7384a99b83adb68650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0faa9cfb3044a7b1b29180e9ba760e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87456b37ecfd4feaaaa0dd45ba3580ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3401536f2d541e1badfdf40523c1dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2daf692e5cc4100be71b8ee52d885b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9237973acf63473d9a31d7ecb2cfefab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dis/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656a0d510c474a70b58228ee9a6f6af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the documents -> this will take a while: DO NOT USE IT\n",
    "documents_embeddings = []\n",
    "for doc in documents[10:50]:\n",
    "    # print(doc['docid'])\n",
    "    text = doc['text']\n",
    "    embeddings = model.encode(text)\n",
    "    documents_embeddings.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all texts of the collection\n",
    "[doc['text'] for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smartert way to embed the documents\n",
    "documents_embeddings2 = model.encode([doc['text'] for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gpu \n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21875it [08:10, 44.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# embedding the queries -> this will take a while: DO NOT USE IT\n",
    "train_queries_embeddings = []\n",
    "gt_documents = []\n",
    "for _, query in tqdm(train_queries.iterrows()):\n",
    "    embeddings = model.encode(query['query'])\n",
    "    gt_documents.append(query['positive_docs'])\n",
    "    train_queries_embeddings.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the embeddings to numpy arrays\n",
    "documents_embeddings_np = np.array(documents_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_embeddings_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the cosine similarity between the first query and all the documents\n",
    "train_queries_embeddings_reshaped = train_queries_embeddings[0].reshape(1, -1)\n",
    "similarity_scores = cosine_similarity(train_queries_embeddings_reshaped, documents_embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index of the 10 most similar documents\n",
    "most_similar_docs = np.argsort(similarity_scores[0])[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30, 978, 993, 833, 668, 503, 900, 457, 807, 583])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc-en-10634',\n",
       " 'doc-en-12492',\n",
       " 'doc-en-3250',\n",
       " 'doc-en-16089',\n",
       " 'doc-en-8017',\n",
       " 'doc-en-5989',\n",
       " 'doc-en-9127',\n",
       " 'doc-en-10148',\n",
       " 'doc-en-9116',\n",
       " 'doc-en-161']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recoveer the docids of the most similar documents\n",
    "most_similar_docids = [documents[i]['docid'] for i in most_similar_docs]\n",
    "most_similar_docids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save documents embeddings\n",
    "np.save('documents_embeddings.npy', documents_embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2= np.load('documents_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 768)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create query embeddings of the test set\n",
    "test_queries_path = '../dis-project-1-document-retrieval/test.csv'\n",
    "test_queries = pd.read_csv(test_queries_path)\n",
    "test_queries_input = [query['query'] for _, query in test_queries.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_queries_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(test_queries_input)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dis/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:630\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[1;32m    629\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m--> 630\u001b[0m                 embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    632\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    634\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Embedding the test queries\n",
    "test_queries_embeddings = model.encode(test_queries_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO NOT USE IT -> it will take a while\n",
    "# def create_submission(queries_embeddings, documents_embeddings):\n",
    "#     \"\"\"\n",
    "#     Create a submission file with the 10 most similar documents for each query\n",
    "#     \"\"\"\n",
    "#     submission = []\n",
    "#     for i, query_embeddings in enumerate(queries_embeddings):\n",
    "#         similarity_scores = cosine_similarity(query_embeddings.reshape(1, -1), documents_embeddings)\n",
    "#         most_similar_docs = np.argsort(similarity_scores[0])[-10:]\n",
    "#         most_similar_docids = [documents[i]['docid'] for i in most_similar_docs]\n",
    "#         submission.append({\n",
    "#             'query_id': i,\n",
    "#             'retrieved_docs': most_similar_docids\n",
    "#         })\n",
    "    \n",
    "#     # save the submission file in csv\n",
    "#     submission_df = pd.DataFrame(submission)\n",
    "#     submission_df.to_csv('submission.csv', index=False)\n",
    "#     print('Submission file created successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and merge the embeddings of the documents\n",
    "documents_embeddings1 = np.load('/Users/giudittadelsarto/Desktop/DIS_Project/doc_embeddings/documents_embeddings_23795.npy')\n",
    "documents_embeddings2 = np.load('/Users/giudittadelsarto/Desktop/DIS_Project/doc_embeddings/documents_embeddings2.npy')\n",
    "documents_embeddings = np.concatenate((documents_embeddings1, documents_embeddings2), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the merged embeddings\n",
    "np.save('full_documents_embeddings.npy', documents_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test queries embeddings\n",
    "test_queries_embeddings = np.load('/Users/giudittadelsarto/Desktop/DIS_Project/doc_embeddings/test_queries_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 768)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_queries_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cosine similarity between the test queries and the documents without for loop\n",
    "sim_matrix = cosine_similarity(test_queries_embeddings, documents_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 268022)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268022,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(queries_embeddings, documents_embeddings):\n",
    "    \"\"\"\n",
    "    Create a submission file with the 10 most similar documents for each query\n",
    "    \"\"\"\n",
    "    submission = []\n",
    "    sim_matrix = cosine_similarity(queries_embeddings, documents_embeddings)\n",
    "    for i in range(len(queries_embeddings)):\n",
    "        most_similar_docs = np.argsort(sim_matrix[i])[-10:]\n",
    "        most_similar_docids = [documents[i]['docid'] for i in most_similar_docs]\n",
    "        submission.append({\n",
    "            'query_id': i,\n",
    "            'retrieved_docs': most_similar_docids\n",
    "        })\n",
    "    \n",
    "    # save the submission file in csv\n",
    "    submission_df = pd.DataFrame(submission)\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print('Submission file created successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_submission(test_queries_embeddings, documents_embeddings)\n",
      "Cell \u001b[0;32mIn[61], line 9\u001b[0m, in \u001b[0;36mcreate_submission\u001b[0;34m(queries_embeddings, documents_embeddings)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(queries_embeddings)):\n\u001b[1;32m      8\u001b[0m     most_similar_docs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(sim_matrix[i])[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m:]\n\u001b[0;32m----> 9\u001b[0m     most_similar_docids \u001b[38;5;241m=\u001b[39m [documents[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m most_similar_docs]\n\u001b[1;32m     10\u001b[0m     submission\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m'\u001b[39m: i,\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretrieved_docs\u001b[39m\u001b[38;5;124m'\u001b[39m: most_similar_docids\n\u001b[1;32m     13\u001b[0m     })\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# save the submission file in csv\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "create_submission(test_queries_embeddings, documents_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide documents embeddings according to language of orginal document\n",
    "\n",
    "embeddings_by_language = {}\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    lang = doc['lang']\n",
    "    if lang not in embeddings_by_language:\n",
    "        embeddings_by_language[lang] = []\n",
    "    embeddings = documents_embeddings[i]\n",
    "    embeddings_by_language[lang].append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute when language changes in test queries\n",
    "\n",
    "breaking_points = [0]\n",
    "for i in range(1, len(test_queries)):\n",
    "    if test_queries.iloc[i]['lang'] != test_queries.iloc[i-1]['lang']:\n",
    "        breaking_points.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_bylang(queries_embeddings, embeddings_by_lang, breaking_points):\n",
    "    \"\"\"\n",
    "    Create a submission file with the 10 most similar documents for each query, considering the language of the query\n",
    "    \"\"\"\n",
    "    submission = []\n",
    "    for i in range(len(breaking_points)):\n",
    "        start = breaking_points[i]\n",
    "        end = breaking_points[i+1] if i+1 < len(breaking_points) else len(queries_embeddings)\n",
    "        queries = queries_embeddings[start:end]\n",
    "        lang = test_queries.iloc[start]['lang']\n",
    "        sim_matrix = cosine_similarity(queries, embeddings_by_lang[lang])\n",
    "        for j in range(len(queries)):\n",
    "            most_similar_docs = np.argsort(sim_matrix[j])[-10:]\n",
    "            most_similar_docids = [documents[i]['docid'] for i in most_similar_docs]\n",
    "            submission.append({\n",
    "                'id': start+j,\n",
    "                'docids': most_similar_docids\n",
    "            })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
